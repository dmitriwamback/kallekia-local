input_ids = tokenizer.encode(prompt, return_tensors="pt")
generated_ids = input_ids[0].tolist()
max_generated_tokens = 100

current_text = prompt  # Keep track of generated words
buffer = ""  # To store word fragments

for _ in range(max_generated_tokens):
    outputs = model.generate(
        torch.tensor([generated_ids]),
        max_length=len(generated_ids) + 1,  # Generate only one token at a time
        num_return_sequences=1,
        no_repeat_ngram_size=3,
        repetition_penalty=1.5,
        do_sample=True,
        top_k=50,
        top_p=nucl,
        temperature=float(temp),
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )

    new_token = outputs[0, -1].item()
    if new_token == tokenizer.eos_token_id:
        break  # Stop if EOS token is reached

    generated_ids.append(new_token)

    # Decode token but do not send immediately
    new_text = tokenizer.decode([new_token], skip_special_tokens=True)
    buffer += new_text

    # Only emit a word when we detect a space (to avoid partial words)
    word = buffer
    current_text += " " + word
    socketio.emit('message', {'data': new_text})
    print(word.strip())  # Debugging

return jsonify({'generated_text': current_text})